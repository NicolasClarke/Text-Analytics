{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modeling\n",
    "Prepared by: Yifan Ren, Ricardo Lu, and Dr. Yilu Zhou"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to Lab 4: Topic Modeling. This will be the last lab of the semester. We are going to talk about 3 latent methods for <b>dimension reduction</b> and <b>topic modeling</b>：\n",
    "1. Latent Semantic Analysis (LSA or LSI)\n",
    "2. Latent Dirichlet Allocation (LDA)\n",
    "3. Correlated LDA Topic Model (Optional)\n",
    "\n",
    "\n",
    "Hightly recommend you go through the link to learn more about both models: https://towardsdatascience.com/2-latent-methods-for-dimension-reduction-and-topic-modeling-20ff6d7d547\n",
    "\n",
    "In the same folder, we provide a regular expression ipython file for your reference. Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import gensim\n",
    "from gensim import corpora,models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>season</th>\n",
       "      <th>brand</th>\n",
       "      <th>author of review</th>\n",
       "      <th>location</th>\n",
       "      <th>time</th>\n",
       "      <th>review text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2016</td>\n",
       "      <td>Spring</td>\n",
       "      <td>A Dtacher</td>\n",
       "      <td>Kristin Anderson</td>\n",
       "      <td>NEW YORK</td>\n",
       "      <td>September 13, 2015</td>\n",
       "      <td>Detachment was the word of the day at A Dtache...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2016</td>\n",
       "      <td>Spring</td>\n",
       "      <td>A.F. Vandevorst</td>\n",
       "      <td>Luke Leitch</td>\n",
       "      <td>PARIS</td>\n",
       "      <td>October 1, 2015</td>\n",
       "      <td>You heard this collection coming long before y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2016</td>\n",
       "      <td>Spring</td>\n",
       "      <td>A.L.C.</td>\n",
       "      <td>Kristin Anderson</td>\n",
       "      <td>NEW YORK</td>\n",
       "      <td>September 21, 2015</td>\n",
       "      <td>August saw the announcement of big news for A....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>Spring</td>\n",
       "      <td>A.P.C.</td>\n",
       "      <td>Nicole Phelps</td>\n",
       "      <td>PARIS</td>\n",
       "      <td>October 3, 2015</td>\n",
       "      <td>They call me the king of basics, Jean Touitou ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2016</td>\n",
       "      <td>Spring</td>\n",
       "      <td>A.W.A.K.E.</td>\n",
       "      <td>Maya Singer</td>\n",
       "      <td>NEW YORK</td>\n",
       "      <td>October 21, 2015</td>\n",
       "      <td>Natalia Alaverdian is a designer with a lot of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>2016</td>\n",
       "      <td>Spring</td>\n",
       "      <td>Zo Jordan</td>\n",
       "      <td>Maya Singer</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>September 19, 2015</td>\n",
       "      <td>Water, water, everywhere, / nor any drop to dr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430</th>\n",
       "      <td>2016</td>\n",
       "      <td>Spring</td>\n",
       "      <td>Zuhair Murad</td>\n",
       "      <td>Amy Verner</td>\n",
       "      <td>PARIS</td>\n",
       "      <td>October 4, 2015</td>\n",
       "      <td>From a new Paris showroom, Zuhair Murad came a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>2016</td>\n",
       "      <td>Spring</td>\n",
       "      <td>1205</td>\n",
       "      <td>Luke Leitch</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>September 19, 2015</td>\n",
       "      <td>Fashion and Instagram are such (often sacchari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>432</th>\n",
       "      <td>2016</td>\n",
       "      <td>Spring</td>\n",
       "      <td>3.1 Phillip Lim</td>\n",
       "      <td>Maya Singer</td>\n",
       "      <td>NEW YORK</td>\n",
       "      <td>September 14, 2015</td>\n",
       "      <td>Let other New York City fashion designers toas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>2016</td>\n",
       "      <td>Spring</td>\n",
       "      <td>6397</td>\n",
       "      <td>Kristin Anderson</td>\n",
       "      <td>NEW YORK</td>\n",
       "      <td>September 14, 2015</td>\n",
       "      <td>Flower power? Yep, the power to make a convert...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>434 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     year   season            brand  author of review  location  \\\n",
       "0     2016  Spring        A Dtacher  Kristin Anderson  NEW YORK   \n",
       "1     2016  Spring  A.F. Vandevorst       Luke Leitch     PARIS   \n",
       "2     2016  Spring           A.L.C.  Kristin Anderson  NEW YORK   \n",
       "3     2016  Spring           A.P.C.     Nicole Phelps     PARIS   \n",
       "4     2016  Spring       A.W.A.K.E.       Maya Singer  NEW YORK   \n",
       "..     ...     ...              ...               ...       ...   \n",
       "429   2016  Spring        Zo Jordan       Maya Singer    LONDON   \n",
       "430   2016  Spring     Zuhair Murad        Amy Verner     PARIS   \n",
       "431   2016  Spring             1205       Luke Leitch    LONDON   \n",
       "432   2016  Spring  3.1 Phillip Lim       Maya Singer  NEW YORK   \n",
       "433   2016  Spring             6397  Kristin Anderson  NEW YORK   \n",
       "\n",
       "                    time                                        review text  \n",
       "0     September 13, 2015  Detachment was the word of the day at A Dtache...  \n",
       "1        October 1, 2015  You heard this collection coming long before y...  \n",
       "2     September 21, 2015  August saw the announcement of big news for A....  \n",
       "3        October 3, 2015  They call me the king of basics, Jean Touitou ...  \n",
       "4       October 21, 2015  Natalia Alaverdian is a designer with a lot of...  \n",
       "..                   ...                                                ...  \n",
       "429   September 19, 2015  Water, water, everywhere, / nor any drop to dr...  \n",
       "430      October 4, 2015  From a new Paris showroom, Zuhair Murad came a...  \n",
       "431   September 19, 2015  Fashion and Instagram are such (often sacchari...  \n",
       "432   September 14, 2015  Let other New York City fashion designers toas...  \n",
       "433   September 14, 2015  Flower power? Yep, the power to make a convert...  \n",
       "\n",
       "[434 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "# use read_csv to read csv file, not read_table\n",
    "df = pd.read_csv('fashion.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Detachment was the word of the day at A Dtacher (yes, like the labels name, bien sr). Designer Mona Kowalska loves the high concept, and one imagines that today detachment included being unconcerned with the gaze of others. Kowalskas woman, both as she appears on the runway and the real world, dresses for herself. Her intensely arty bend, and taste for clothes that match it, make A Dtacher a cultishly beloved brand among certain shoppers. This season, Kowalska presented them with a lineup of relatively playful offerings.\\rThe collection opened with a pair of midi dresses in an Indonesian-inspired floral print, which reemerged later imagined with allover Pop white polka dots. Elsewhere came cardigans in an uncanny kind of amoxicillin pink that you imagined the A Dtacher woman wearing with tongue firmly in cheek (they had Kawakubo-esque allover holes, to boot). The popcorn knits were pretty fun, too.\\rThe choice to use hardier materials lent dresses eccentric volumes, but also led to a lineup that often felt frumpy, albeit fashionably so. At times the clothes lacked the excitement and want-it-now appeal of Kowalskas Spring collection a year ago. Still, when you looked around the room at todays show and spotted women from all walks of life wearing A Dtacher clothes from various seasons gone by, you could be sure theyd find plenty to like here, too.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert all review text into list format\n",
    "docs = df['review text'].tolist()\n",
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the documents.\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Split the documents into tokens.\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "for idx in range(len(docs)):\n",
    "    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n",
    "    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n",
    "\n",
    "# Remove numbers, but not words that contain numbers.\n",
    "docs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n",
    "    \n",
    "# Remove stopwords.\n",
    "docs = [[token for token in doc if token not in stopwords.words('english')] for doc in docs]\n",
    "\n",
    "# Remove words that are only one character.\n",
    "docs = [[token for token in doc if len(token) > 1] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize the documents.\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute bigrams.\n",
    "from gensim.models import Phrases\n",
    "\n",
    "# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\n",
    "bigram = Phrases(docs, min_count=10)\n",
    "for idx in range(len(docs)):\n",
    "    for token in bigram[docs[idx]]:\n",
    "        if '_' in token:\n",
    "            # Token is a bigram, add to document.\n",
    "            docs[idx].append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rare and common tokens.\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "# Create a dictionary representation of the documents.\n",
    "dictionary = Dictionary(docs)\n",
    "\n",
    "# Filter out words that occur less than 10 documents, or more than 70% of the documents.\n",
    "# This step would be necessary in larger text\n",
    "# dictionary.filter_extremes(no_below=10, no_above=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Term Document Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique tokens: 13812\n",
      "Number of documents: 434\n"
     ]
    }
   ],
   "source": [
    "# Bag-of-words representation of the documents.\n",
    "corpus = [dictionary.doc2bow(doc) for doc in docs]\n",
    "\n",
    "print('Number of unique tokens: %d' % len(dictionary))\n",
    "print('Number of documents: %d' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a unique token list \n",
    "sort_token = sorted(dictionary.items(),key=lambda k:k[0], reverse = False)\n",
    "unique_token = [token.encode('utf8') for (ID,token) in sort_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "matrix = gensim.matutils.corpus2dense(corpus,num_terms=len(dictionary),dtype = 'int')\n",
    "matrix = matrix.T #transpose the matrix \n",
    "\n",
    "#convert the numpy matrix into pandas data frame\n",
    "matrix_df = pd.DataFrame(matrix, columns=unique_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write matrix dataframe into csv\n",
    "matrix_df.to_csv('Term_Document_matrix.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LDA model.\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Set training parameters.\n",
    "num_topics = 10\n",
    "chunksize = 2000\n",
    "passes = 20\n",
    "iterations = 100\n",
    "eval_every = 1  # Don't evaluate model perplexity, takes too much time.\n",
    "\n",
    "# Make a index to word dictionary.\n",
    "temp = dictionary[0]  # This is only to \"load\" the dictionary.\n",
    "id2word = dictionary.id2token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #1:\n",
      "\"collection\",\"dress\",\"one\",\"show\",\"season\",\"new\",\"look\",\"year\",\"designer\",\"way\"\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "\"dress\",\"designer\",\"one\",\"like\",\"collection\",\"show\",\"look\",\"season\",\"woman\",\"spring\"\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "\"dress\",\"collection\",\"like\",\"show\",\"designer\",\"look\",\"one\",\"woman\",\"new\",\"said\"\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "\"new\",\"dress\",\"designer\",\"like\",\"one\",\"collection\",\"show\",\"season\",\"spring\",\"look\"\n",
      "\n",
      "\n",
      "Top 10 words for topic #5:\n",
      "\"dress\",\"collection\",\"one\",\"fashion\",\"look\",\"designer\",\"said\",\"like\",\"spring\",\"clothes\"\n",
      "\n",
      "\n",
      "Top 10 words for topic #6:\n",
      "\"collection\",\"dress\",\"like\",\"designer\",\"look\",\"new\",\"one\",\"piece\",\"skirt\",\"also\"\n",
      "\n",
      "\n",
      "Top 10 words for topic #7:\n",
      "\"collection\",\"look\",\"anderson\",\"dress\",\"said\",\"black\",\"pant\",\"new\",\"lee\",\"bag\"\n",
      "\n",
      "\n",
      "Top 10 words for topic #8:\n",
      "\"collection\",\"one\",\"dress\",\"piece\",\"season\",\"designer\",\"show\",\"clothes\",\"like\",\"look\"\n",
      "\n",
      "\n",
      "Top 10 words for topic #9:\n",
      "\"collection\",\"dress\",\"look\",\"show\",\"designer\",\"new\",\"one\",\"print\",\"like\",\"way\"\n",
      "\n",
      "\n",
      "Top 10 words for topic #10:\n",
      "\"dress\",\"collection\",\"look\",\"one\",\"like\",\"show\",\"print\",\"black\",\"new\",\"designer\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lda = LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    chunksize=chunksize,\n",
    "    alpha='auto',\n",
    "    eta='auto',\n",
    "    iterations=iterations,\n",
    "    num_topics=num_topics,\n",
    "    passes=passes,\n",
    "    eval_every=eval_every\n",
    ")\n",
    "\n",
    "lda.print_topics(10) #V matrix, topic matrix\n",
    "import re\n",
    "for i,topic in lda.print_topics(10):\n",
    "    print(f'Top 10 words for topic #{i+1}:')\n",
    "    print(\",\".join(re.findall('\".*?\"',topic)))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lda' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-5338ecc94adb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtop_topics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop_topics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#, num_words=20)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mavg_topic_coherence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtop_topics\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Average topic coherence: %.4f.'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mavg_topic_coherence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lda' is not defined"
     ]
    }
   ],
   "source": [
    "top_topics = lda.top_topics(corpus) #, num_words=20)\n",
    "\n",
    "# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\n",
    "avg_topic_coherence = sum([t[1] for t in top_topics]) / num_topics\n",
    "print('Average topic coherence: %.4f.' % avg_topic_coherence)\n",
    "\n",
    "from pprint import pprint\n",
    "pprint(top_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate U Matrix for LDA model\n",
    "corpus_lda = lda[corpus] #transform lda model\n",
    "\n",
    "#convert corpus_lda to numpy matrix\n",
    "U_matrix_lda = gensim.matutils.corpus2dense(corpus_lda,num_terms=10).T\n",
    "\n",
    "#write U_matrix into pandas dataframe and output\n",
    "U_matrix_lda_df = pd.DataFrame(U_matrix_lda)\n",
    "U_matrix_lda_df.to_csv('U_matrix_lda.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(434, 13812)\n",
      "(434, 10)\n"
     ]
    }
   ],
   "source": [
    "print(matrix_df.shape)\n",
    "print(U_matrix_lda_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See what we have achieved! We decrease features from 7493 to 10!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSI model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tfidf Transformation \n",
    "tfidf = models.TfidfModel(corpus) #fit tfidf model\n",
    "corpus_tfidf = tfidf[corpus]      #transform tfidf model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #1:\n",
      "\"show\",\"new\",\"woman\",\"season\",\"print\",\"silk\",\"white\",\"brand\",\"black\",\"jacket\"\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "\"versace\",\"show\",\"model\",\"fashion\",\"cotton\",\"denim\",\"jumpsuit\",\"people\",\"knit\",\"graphic\"\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "\"denim\",\"jean\",\"osborne\",\"brand\",\"dkny\",\"chow\",\"gown\",\"vintage\",\"red\",\"black_white\"\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "\"lee\",\"johnson\",\"biker\",\"he\",\"anderson\",\"gown\",\"valli\",\"shoulder\",\"flower\",\"jacket\"\n",
      "\n",
      "\n",
      "Top 10 words for topic #5:\n",
      "\"lee\",\"lim\",\"shirt\",\"webb\",\"jean\",\"pop\",\"pom\",\"twist\",\"taylor\",\"japanese\"\n",
      "\n",
      "\n",
      "Top 10 words for topic #6:\n",
      "\"osborne\",\"chow\",\"dkny\",\"wang\",\"giorgetti\",\"lee\",\"denim\",\"walker\",\"jean\",\"lim\"\n",
      "\n",
      "\n",
      "Top 10 words for topic #7:\n",
      "\"osborne\",\"de\",\"johnson\",\"wu\",\"dkny\",\"he\",\"chow\",\"comme\",\"scott\",\"lee\"\n",
      "\n",
      "\n",
      "Top 10 words for topic #8:\n",
      "\"johnson\",\"wang\",\"chiuri\",\"osborne\",\"chow\",\"anderson\",\"piccioli\",\"african\",\"font\",\"dkny\"\n",
      "\n",
      "\n",
      "Top 10 words for topic #9:\n",
      "\"wang\",\"denim\",\"giorgetti\",\"woman\",\"webb\",\"versace\",\"pom\",\"taits\",\"sweater\",\"reference\"\n",
      "\n",
      "\n",
      "Top 10 words for topic #10:\n",
      "\"versace\",\"osborne\",\"chow\",\"dkny\",\"de\",\"johnson\",\"lee\",\"donatella\",\"de_la\",\"webb\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train LSI model.\n",
    "from gensim.models import LsiModel\n",
    "\n",
    "\n",
    "lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=10)\n",
    "\n",
    "import re\n",
    "for i,topic in lsi.print_topics(10):\n",
    "    print(f'Top 10 words for topic #{i+1}:')\n",
    "    print(\",\".join(re.findall('\".*?\"',topic)))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate U Matrix for LSI model\n",
    "corpus_lsi = lsi[corpus_tfidf] #transform lda model\n",
    "\n",
    "#convert corpus_lsi to numpy matrix\n",
    "U_matrix_lsi = gensim.matutils.corpus2dense(corpus_lsi,num_terms=10).T\n",
    "\n",
    "#write U_matrix into pandas dataframe and output\n",
    "pd.DataFrame(U_matrix_lsi).to_csv('U_matrix_lsi.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlated LDA Topic Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tomotopy\n",
      "  Downloading tomotopy-0.11.1-cp38-cp38-macosx_10_14_x86_64.whl (13.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 13.7 MB 3.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /Users/nicolasclarke/opt/anaconda3/lib/python3.8/site-packages (from tomotopy) (1.19.2)\n",
      "Installing collected packages: tomotopy\n",
      "Successfully installed tomotopy-0.11.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tomotopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tomotopy as tp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ctm = tp.CTModel(k=10)\n",
    "for doc in docs:\n",
    "    ctm.add_doc(doc)\n",
    "for i in range(0, 500, 10):\n",
    "    ctm.train(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_matrix_lda_df = pd.DataFrame([doc.get_topic_dist() for doc in ctm.docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "imitate_print = lambda ctm:[(i,\" + \".join([str(round(p,3))+\"*\"+'\"{}\"'.format(w) for w,p in ctm.get_topic_words(i)])) for i in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 words for topic #1:\n",
      "\"also\",\"back\",\"runway\",\"high\",\"fabric\",\"make\",\"denim\",\"work\",\"gown\",\"suit\"\n",
      "\n",
      "\n",
      "Top 10 words for topic #2:\n",
      "\"black\",\"would\",\"long\",\"around\",\"see\",\"feel\",\"little\",\"idea\",\"always\",\"short\"\n",
      "\n",
      "\n",
      "Top 10 words for topic #3:\n",
      "\"show\",\"spring\",\"shirt\",\"girl\",\"coat\",\"time\",\"used\",\"material\",\"perhaps\",\"often\"\n",
      "\n",
      "\n",
      "Top 10 words for topic #4:\n",
      "\"made\",\"first\",\"woman\",\"style\",\"point\",\"go\",\"many\",\"trouser\",\"body\",\"floral\"\n",
      "\n",
      "\n",
      "Top 10 words for topic #5:\n",
      "\"season\",\"way\",\"silk\",\"brand\",\"well\",\"hand\",\"york\",\"label\",\"new_york\",\"best\"\n",
      "\n",
      "\n",
      "Top 10 words for topic #6:\n",
      "\"new\",\"said\",\"knit\",\"day\",\"though\",\"leather\",\"time\",\"felt\",\"really\",\"something\"\n",
      "\n",
      "\n",
      "Top 10 words for topic #7:\n",
      "\"like\",\"piece\",\"fashion\",\"pant\",\"today\",\"line\",\"cut\",\"take\",\"inspired\",\"backstage\"\n",
      "\n",
      "\n",
      "Top 10 words for topic #8:\n",
      "\"one\",\"designer\",\"white\",\"could\",\"even\",\"model\",\"came\",\"thats\",\"another\",\"bit\"\n",
      "\n",
      "\n",
      "Top 10 words for topic #9:\n",
      "\"dress\",\"collection\",\"print\",\"clothes\",\"jacket\",\"theme\",\"cotton\",\"thing\",\"le\",\"come\"\n",
      "\n",
      "\n",
      "Top 10 words for topic #10:\n",
      "\"look\",\"skirt\",\"top\",\"year\",\"lace\",\"silhouette\",\"red\",\"wear\",\"worn\",\"thing\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "for i,topic in imitate_print(ctm):\n",
    "    print(f'Top 10 words for topic #{i+1}:')\n",
    "    print(\",\".join(re.findall('\".*?\"',topic)))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "U_matrix_lda_df.to_csv('U_matrix_ctm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
