{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('News_Article_2016_Jan.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Day</th>\n",
       "      <th>Name</th>\n",
       "      <th>Year</th>\n",
       "      <th>month</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21</td>\n",
       "      <td>3D SYSTEMS</td>\n",
       "      <td>2016</td>\n",
       "      <td>January</td>\n",
       "      <td>Plastics ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>3D SYSTEMS</td>\n",
       "      <td>2016</td>\n",
       "      <td>January</td>\n",
       "      <td>US Officia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>3D SYSTEMS</td>\n",
       "      <td>2016</td>\n",
       "      <td>January</td>\n",
       "      <td>Progressive Media ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3D SYSTEMS</td>\n",
       "      <td>2016</td>\n",
       "      <td>January</td>\n",
       "      <td>Stock Wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>3D SYSTEMS</td>\n",
       "      <td>2016</td>\n",
       "      <td>January</td>\n",
       "      <td>India Automobi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19644</th>\n",
       "      <td>16</td>\n",
       "      <td>GOOGLE</td>\n",
       "      <td>2016</td>\n",
       "      <td>January</td>\n",
       "      <td>The Times of In...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19645</th>\n",
       "      <td>23</td>\n",
       "      <td>GOOGLE</td>\n",
       "      <td>2016</td>\n",
       "      <td>January</td>\n",
       "      <td>National Post's Financial Post...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19646</th>\n",
       "      <td>30</td>\n",
       "      <td>GOOGLE</td>\n",
       "      <td>2016</td>\n",
       "      <td>January</td>\n",
       "      <td>Stock Wat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19647</th>\n",
       "      <td>1</td>\n",
       "      <td>GOOGLE</td>\n",
       "      <td>2016</td>\n",
       "      <td>January</td>\n",
       "      <td>Zee Ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19648</th>\n",
       "      <td>23</td>\n",
       "      <td>GOOGLE</td>\n",
       "      <td>2016</td>\n",
       "      <td>January</td>\n",
       "      <td>The Journal (Newc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19649 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Day        Name  Year    month  \\\n",
       "0       21  3D SYSTEMS  2016  January   \n",
       "1       11  3D SYSTEMS  2016  January   \n",
       "2       12  3D SYSTEMS  2016  January   \n",
       "3        1  3D SYSTEMS  2016  January   \n",
       "4       18  3D SYSTEMS  2016  January   \n",
       "...    ...         ...   ...      ...   \n",
       "19644   16      GOOGLE  2016  January   \n",
       "19645   23      GOOGLE  2016  January   \n",
       "19646   30      GOOGLE  2016  January   \n",
       "19647    1      GOOGLE  2016  January   \n",
       "19648   23      GOOGLE  2016  January   \n",
       "\n",
       "                                                 Content  \n",
       "0                                           Plastics ...  \n",
       "1                                          US Officia...  \n",
       "2                                  Progressive Media ...  \n",
       "3                                           Stock Wat...  \n",
       "4                                      India Automobi...  \n",
       "...                                                  ...  \n",
       "19644                                 The Times of In...  \n",
       "19645                  National Post's Financial Post...  \n",
       "19646                                       Stock Wat...  \n",
       "19647                                          Zee Ne...  \n",
       "19648                               The Journal (Newc...  \n",
       "\n",
       "[19649 rows x 5 columns]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NETFLIX                     123\n",
       "HNI                         106\n",
       "HAWKINS                      98\n",
       "AMERICAN INTERNATIONAL       88\n",
       "GRIFFON                      84\n",
       "HCP                          78\n",
       "GUESS                        76\n",
       "FTI CONSULTING               76\n",
       "MATTEL                       75\n",
       "TWENTY-FIRST CENTURY FOX     74\n",
       "GREEN DOT                    72\n",
       "GRACO                        71\n",
       "GREENHILL & CO               70\n",
       "UNDER ARMOUR                 68\n",
       "HEALTHCARE SERVICES          68\n",
       "HUNTINGTON BANCSHARES        68\n",
       "TIFFANY                      65\n",
       "HASBRO                       64\n",
       "CHIPOTLE MEXICAN GRILL       63\n",
       "ANTHEM                       62\n",
       "Name: Name, dtype: int64"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"Name\"].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name each week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "week1=list(range(1,8))\n",
    "week2=list(range(8,15))\n",
    "week3=list(range(15,22))\n",
    "week4=list(range(22,29))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify When 'Mattel' appears during each week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mattel Week 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mattel_week1 = df[df['Day'].isin(week1) & (df['Name']== 'MATTEL')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_list_week1 = Mattel_week1['Content']\n",
    "raw = ' '.join(content_list_week1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10212                                        US Officia...\n",
       "10230                                        The Austra...\n",
       "Name: Content, dtype: object"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_list_week1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(raw)\n",
    "\n",
    "mystopwords = stopwords.words('english')\n",
    "words = [w.lower() for w in tokens if w.isalpha() if w.lower()not in mystopwords]\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "stem1 = [porter.stem(w) for w in words]\n",
    "\n",
    "#Encode with utf-8\n",
    "stem1 = [w for w in stem1]\n",
    "\n",
    "#Get the frequency distribution \n",
    "freq1 = FreqDist(stem1)\n",
    "#Sort the result\n",
    "sorted_freq1 = sorted(freq1.items(),key = lambda k: k[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mattel Week 1 Top 10 words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['fund',\n",
       " 'great',\n",
       " 'white',\n",
       " 'children',\n",
       " 'fuhu',\n",
       " 'januari',\n",
       " 'shark',\n",
       " 'tablet',\n",
       " 'norman',\n",
       " 'offer']"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mattel_week1_top10 = [i[0] for i in sorted_freq1][:10]\n",
    "print (\"Mattel Week 1 Top 10 words\")\n",
    "mattel_week1_top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mattel Week 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mattel_week2 = df[df['Day'].isin(week2) & (df['Name']== 'MATTEL')]\n",
    "content_list_week2 = Mattel_week2['Content']\n",
    "raw2 = ' '.join(content_list_week2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(raw2)\n",
    "\n",
    "mystopwords = stopwords.words('english')\n",
    "words = [w.lower() for w in tokens if w.isalpha() if w.lower()not in mystopwords]\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "stem2 = [porter.stem(w) for w in words]\n",
    "\n",
    "#Encode with utf-8\n",
    "stem2 = [w for w in stem2]\n",
    "\n",
    "#Get the frequency distribution \n",
    "freq2 = FreqDist(stem2)\n",
    "#Sort the result\n",
    "sorted_freq2 = sorted(freq2.items(),key = lambda k: k[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mattel Week 2 Top 10 words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['mattel',\n",
       " 'play',\n",
       " 'liverpool',\n",
       " 'barbi',\n",
       " 'attract',\n",
       " 'januari',\n",
       " 'toy',\n",
       " 'children',\n",
       " 'open',\n",
       " 'section']"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mattel_week2_top10 = [i[0] for i in sorted_freq2][:10]\n",
    "print (\"Mattel Week 2 Top 10 words\")\n",
    "mattel_week2_top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mattel Week 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mattel_week3 = df[df['Day'].isin(week3) & (df['Name']== 'MATTEL')]\n",
    "content_list_week3 = Mattel_week3['Content']\n",
    "raw3 = ' '.join(content_list_week3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(raw3)\n",
    "\n",
    "mystopwords = stopwords.words('english')\n",
    "words = [w.lower() for w in tokens if w.isalpha() if w.lower()not in mystopwords]\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "stem3 = [porter.stem(w) for w in words]\n",
    "\n",
    "#Encode with utf-8\n",
    "stem3 = [w for w in stem3]\n",
    "\n",
    "#Get the frequency distribution \n",
    "freq3 = FreqDist(stem3)\n",
    "#Sort the result\n",
    "sorted_freq3 = sorted(freq3.items(),key = lambda k: k[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mattel Week 3 Top 10 words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['children',\n",
       " 'mattel',\n",
       " 'hospit',\n",
       " 'beij',\n",
       " 'barbi',\n",
       " 'hacker',\n",
       " 'foundat',\n",
       " 'group',\n",
       " 'banga',\n",
       " 'doll']"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mattel_week3_top10 = [i[0] for i in sorted_freq3][:10]\n",
    "print (\"Mattel Week 3 Top 10 words\")\n",
    "mattel_week3_top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mattel Week 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mattel_week4 = df[df['Day'].isin(week4) & (df['Name']== 'MATTEL')]\n",
    "content_list_week4 = Mattel_week4['Content']\n",
    "raw4 = ' '.join(content_list_week4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(raw4)\n",
    "\n",
    "mystopwords = stopwords.words('english')\n",
    "words = [w.lower() for w in tokens if w.isalpha() if w.lower()not in mystopwords]\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "stem4 = [porter.stem(w) for w in words]\n",
    "\n",
    "#Encode with utf-8\n",
    "stem4 = [w for w in stem4]\n",
    "\n",
    "#Get the frequency distribution \n",
    "freq4 = FreqDist(stem4)\n",
    "#Sort the result\n",
    "sorted_freq4 = sorted(freq4.items(),key = lambda k: k[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mattel Week 4 Top 10 words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['barbi',\n",
       " 'doll',\n",
       " 'mattel',\n",
       " 'bodi',\n",
       " 'new',\n",
       " 'girl',\n",
       " 'said',\n",
       " 'januari',\n",
       " 'world',\n",
       " 'year']"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mattel_week4_top10 = [i[0] for i in sorted_freq4][:10]\n",
    "print (\"Mattel Week 4 Top 10 words\")\n",
    "mattel_week4_top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify When 'HASBRO' appears during each week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hasbro Week 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hasbro_week1 = df[df['Day'].isin(week1) & (df['Name']== 'HASBRO')]\n",
    "content_list_week1H = Hasbro_week1['Content']\n",
    "raw5 = ' '.join(content_list_week1H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(raw5)\n",
    "\n",
    "mystopwords = stopwords.words('english')\n",
    "words = [w.lower() for w in tokens if w.isalpha() if w.lower()not in mystopwords]\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "stem5 = [porter.stem(w) for w in words]\n",
    "\n",
    "#Encode with utf-8\n",
    "stem5 = [w for w in stem5]\n",
    "\n",
    "#Get the frequency distribution \n",
    "freq5 = FreqDist(stem5)\n",
    "#Sort the result\n",
    "sorted_freq5 = sorted(freq5.items(),key = lambda k: k[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasbro Week 1 Top 10 words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rey',\n",
       " 'star',\n",
       " 'war',\n",
       " 'game',\n",
       " 'hasbro',\n",
       " 'charact',\n",
       " 'monopoli',\n",
       " 'film',\n",
       " 'fan',\n",
       " 'awaken']"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HASBRO_week1_top10 = [i[0] for i in sorted_freq5][:10]\n",
    "print (\"Hasbro Week 1 Top 10 words\")\n",
    "HASBRO_week1_top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hasbro Week 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hasbro_week2 = df[df['Day'].isin(week2) & (df['Name']== 'HASBRO')]\n",
    "content_list_week2H = Hasbro_week2['Content']\n",
    "raw6 = ' '.join(content_list_week2H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(raw6)\n",
    "\n",
    "mystopwords = stopwords.words('english')\n",
    "words = [w.lower() for w in tokens if w.isalpha() if w.lower()not in mystopwords]\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "stem6 = [porter.stem(w) for w in words]\n",
    "\n",
    "#Encode with utf-8\n",
    "stem6 = [w for w in stem6]\n",
    "\n",
    "#Get the frequency distribution \n",
    "freq6 = FreqDist(stem6)\n",
    "#Sort the result\n",
    "sorted_freq6 = sorted(freq6.items(),key = lambda k: k[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasbro Week 2 Top 10 words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rey',\n",
       " 'hasbro',\n",
       " 'star',\n",
       " 'game',\n",
       " 'war',\n",
       " 'girl',\n",
       " 'charact',\n",
       " 'monopoli',\n",
       " 'januari',\n",
       " 'toy']"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HASBRO_week2_top10 = [i[0] for i in sorted_freq6][:10]\n",
    "print (\"Hasbro Week 2 Top 10 words\")\n",
    "HASBRO_week2_top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hasbro Week 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hasbro_week3 = df[df['Day'].isin(week3) & (df['Name']== 'HASBRO')]\n",
    "content_list_week3H = Hasbro_week3['Content']\n",
    "raw7 = ' '.join(content_list_week3H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(raw7)\n",
    "\n",
    "mystopwords = stopwords.words('english')\n",
    "words = [w.lower() for w in tokens if w.isalpha() if w.lower()not in mystopwords]\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "stem7 = [porter.stem(w) for w in words]\n",
    "\n",
    "#Encode with utf-8\n",
    "stem7 = [w for w in stem7]\n",
    "\n",
    "#Get the frequency distribution \n",
    "freq7 = FreqDist(stem7)\n",
    "#Sort the result\n",
    "sorted_freq7 = sorted(freq7.items(),key = lambda k: k[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasbro Week 3 Top 10 words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['rey',\n",
       " 'toy',\n",
       " 'disney',\n",
       " 'charact',\n",
       " 'merchandis',\n",
       " 'said',\n",
       " 'star',\n",
       " 'war',\n",
       " 'new',\n",
       " 'movi']"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HASBRO_week3_top10 = [i[0] for i in sorted_freq7][:10]\n",
    "print (\"Hasbro Week 3 Top 10 words\")\n",
    "HASBRO_week3_top10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hasbro Week 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "Hasbro_week4 = df[df['Day'].isin(week4) & (df['Name']== 'HASBRO')]\n",
    "content_list_week4H = Hasbro_week4['Content']\n",
    "raw8 = ' '.join(content_list_week4H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(raw8)\n",
    "\n",
    "mystopwords = stopwords.words('english')\n",
    "words = [w.lower() for w in tokens if w.isalpha() if w.lower()not in mystopwords]\n",
    "\n",
    "porter = nltk.PorterStemmer()\n",
    "stem8 = [porter.stem(w) for w in words]\n",
    "\n",
    "#Encode with utf-8\n",
    "stem8 = [w for w in stem8]\n",
    "\n",
    "#Get the frequency distribution \n",
    "freq8 = FreqDist(stem8)\n",
    "#Sort the result\n",
    "sorted_freq8 = sorted(freq8.items(),key = lambda k: k[1], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasbro Week 4 Top 10 words\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['action',\n",
       " 'toy',\n",
       " 'man',\n",
       " 'hasbro',\n",
       " 'ccc',\n",
       " 'januari',\n",
       " 'palitoy',\n",
       " 'bob',\n",
       " 'compani',\n",
       " 'girl']"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HASBRO_week4_top10 = [i[0] for i in sorted_freq8][:10]\n",
    "print (\"Hasbro Week 4 Top 10 words\")\n",
    "HASBRO_week4_top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
