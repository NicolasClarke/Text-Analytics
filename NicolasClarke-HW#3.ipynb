{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import PorterStemmer\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"Spam Email.csv\", usecols=[\"CATEGORY\", \"MESSAGE\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>MESSAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Dear Homeowner,\\n\\n \\n\\nInterest Rates are at ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ATTENTION: This is a MUST for ALL Computer Use...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>This is a multi-part message in MIME format.\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>IMPORTANT INFORMATION:\\n\\n\\n\\nThe new domain n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>This is the bottom line.  If you can GIVE AWAY...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5791</th>\n",
       "      <td>0</td>\n",
       "      <td>I'm one of the 30,000 but it's not working ver...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5792</th>\n",
       "      <td>0</td>\n",
       "      <td>Damien Morton quoted:\\n\\n&gt;W3C approves HTML 4 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5793</th>\n",
       "      <td>0</td>\n",
       "      <td>On Mon, 2002-07-22 at 06:50, che wrote:\\n\\n\\n\\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5794</th>\n",
       "      <td>0</td>\n",
       "      <td>Once upon a time, Manfred wrote :\\n\\n\\n\\n&gt; I w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5795</th>\n",
       "      <td>0</td>\n",
       "      <td>If you run Pick, and then use the \"New FTOC\" b...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5796 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CATEGORY                                            MESSAGE\n",
       "0            1  Dear Homeowner,\\n\\n \\n\\nInterest Rates are at ...\n",
       "1            1  ATTENTION: This is a MUST for ALL Computer Use...\n",
       "2            1  This is a multi-part message in MIME format.\\n...\n",
       "3            1  IMPORTANT INFORMATION:\\n\\n\\n\\nThe new domain n...\n",
       "4            1  This is the bottom line.  If you can GIVE AWAY...\n",
       "...        ...                                                ...\n",
       "5791         0  I'm one of the 30,000 but it's not working ver...\n",
       "5792         0  Damien Morton quoted:\\n\\n>W3C approves HTML 4 ...\n",
       "5793         0  On Mon, 2002-07-22 at 06:50, che wrote:\\n\\n\\n\\...\n",
       "5794         0  Once upon a time, Manfred wrote :\\n\\n\\n\\n> I w...\n",
       "5795         0  If you run Pick, and then use the \"New FTOC\" b...\n",
       "\n",
       "[5796 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5796 entries, 0 to 5795\n",
      "Data columns (total 2 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   CATEGORY  5796 non-null   int64 \n",
      " 1   MESSAGE   5796 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 90.7+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    3900\n",
       "1    1896\n",
       "Name: CATEGORY, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"CATEGORY\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove non alphabets\n",
    "remove_non_alphabets = lambda x: re.sub(r'[^a-zA-Z]',' ',x)\n",
    "\n",
    "# tokenn alphabets-only list\n",
    "tokenize = lambda x: word_tokenize(x)\n",
    "\n",
    "# assign ps to a lambda function to run on each line of value\n",
    "ps = PorterStemmer()\n",
    "stem = lambda w: [ ps.stem(x) for x in w ]\n",
    "\n",
    "# assign lemmatizer to a lambda function to run on each line of value\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "leammtizer = lambda x: [ lemmatizer.lemmatize(word) for word in x ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing : [=====] : Completed"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CATEGORY</th>\n",
       "      <th>MESSAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>dear homeown interest rate are at their lowest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>attent thi is a must for all comput user new s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>thi is a multi part messag in mime format next...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>import inform the new domain name are final av...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>thi is the bottom line If you can give away CD...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CATEGORY                                            MESSAGE\n",
       "0         1  dear homeown interest rate are at their lowest...\n",
       "1         1  attent thi is a must for all comput user new s...\n",
       "2         1  thi is a multi part messag in mime format next...\n",
       "3         1  import inform the new domain name are final av...\n",
       "4         1  thi is the bottom line If you can give away CD..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# apply all above methods to the column ''\n",
    "print('Processing : [=', end='')\n",
    "data['MESSAGE'] = data['MESSAGE'].apply(remove_non_alphabets)\n",
    "print('=', end='')\n",
    "data['MESSAGE'] = data['MESSAGE'].apply(tokenize)\n",
    "print('=', end='')\n",
    "data['MESSAGE'] = data['MESSAGE'].apply(stem)\n",
    "print('=', end='')\n",
    "data['MESSAGE'] = data['MESSAGE'].apply(leammtizer)\n",
    "print('=', end='')\n",
    "data['MESSAGE'] = data['MESSAGE'].apply(lambda x: ' '.join(x))\n",
    "print('] : Completed', end='')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split Train Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus, test_corpus, train_labels, test_labels = train_test_split(data[\"MESSAGE\"],\n",
    "                                                                        data[\"CATEGORY\"],\n",
    "                                                                        test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_vectorizer=CountVectorizer(min_df=1, ngram_range=(1,1))\n",
    "bow_train_features = bow_vectorizer.fit_transform(train_corpus)\n",
    "bow_test_features = bow_vectorizer.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_vectorizer=TfidfVectorizer(min_df=1, \n",
    "                                 norm='l2',\n",
    "                                 smooth_idf=True,\n",
    "                                 use_idf=True,\n",
    "                                 ngram_range=(1,1))\n",
    "tfidf_train_features = tfidf_vectorizer.fit_transform(train_corpus)  \n",
    "tfidf_test_features = tfidf_vectorizer.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_vectorizer=CountVectorizer(binary = True,min_df=1, ngram_range=(1,1))\n",
    "binary_train_features = binary_vectorizer.fit_transform(train_corpus)\n",
    "binary_test_features = binary_vectorizer.transform(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(true_labels, predicted_labels):\n",
    "    metrics_dict = dict(zip([\"accuracy\", \"precision\", \"recall\", \"f1\"], [None]*4))\n",
    "    #metrics_dict = {i:None for i in [\"accuracy\", \"precision\", \"recall\", \"f1\"]}\n",
    "    for m in metrics_dict.keys():\n",
    "        exec('''metrics_dict['{}'] = np.round(                                                    \n",
    "                        metrics.{}_score(true_labels, \n",
    "                                               predicted_labels),\n",
    "                        2)'''.format(m, m))\n",
    "    return metrics_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define an Easy-to-use Function for Train/Test/Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_predict_evaluate_model(classifier, \n",
    "                                 train_features, train_labels, \n",
    "                                 test_features, test_labels):\n",
    "    # build model    \n",
    "    classifier.fit(train_features, train_labels)\n",
    "    # predict using model\n",
    "    predictions = classifier.predict(test_features) \n",
    "    # evaluate model prediction performance   \n",
    "    '''get_metrics(true_labels=test_labels, \n",
    "                predicted_labels=predictions)'''\n",
    "    print(confusion_matrix(test_labels, predictions))\n",
    "    print('\\n')\n",
    "    print(metrics.classification_report(test_labels,predictions))\n",
    "    #return predictions, get_metrics(true_labels=test_labels, predicted_labels=predictions)\n",
    "    return confusion_matrix(test_labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB # import naive bayes\n",
    "from sklearn.tree import DecisionTreeClassifier # import Decision Tree\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test on BOW features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1157    5]\n",
      " [ 144  433]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      1.00      0.94      1162\n",
      "           1       0.99      0.75      0.85       577\n",
      "\n",
      "    accuracy                           0.91      1739\n",
      "   macro avg       0.94      0.87      0.90      1739\n",
      "weighted avg       0.92      0.91      0.91      1739\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "\n",
    "# predict and evaluate naive bayes\n",
    "mnb_bow_predictions, mnb_bow_metrics = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_nb_bow = (mnb_bow_predictions, mnb_bow_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([1157,    5]), array([144, 433]))\n"
     ]
    }
   ],
   "source": [
    "print(matrix_nb_bow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1116   46]\n",
      " [  41  536]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.96      0.96      1162\n",
      "           1       0.92      0.93      0.92       577\n",
      "\n",
      "    accuracy                           0.95      1739\n",
      "   macro avg       0.94      0.94      0.94      1739\n",
      "weighted avg       0.95      0.95      0.95      1739\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# assign decision tree function to an object\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# predict and evaluate decision tree\n",
    "dt_bow_predictions, dt_bow_metrics = train_predict_evaluate_model(classifier=dt,\n",
    "                                                               train_features=bow_train_features,\n",
    "                                                               train_labels=train_labels,\n",
    "                                                               test_features=bow_test_features,\n",
    "                                                               test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_dt_bow = (dt_bow_predictions, dt_bow_metrics )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1158    4]\n",
      " [  38  539]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      1162\n",
      "           1       0.99      0.93      0.96       577\n",
      "\n",
      "    accuracy                           0.98      1739\n",
      "   macro avg       0.98      0.97      0.97      1739\n",
      "weighted avg       0.98      0.98      0.98      1739\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# assign random forest function to an object\n",
    "rf = RandomForestClassifier(criterion=\"entropy\")\n",
    "\n",
    "# predict and evaluate random forest\n",
    "rf_bow_predictions, rf_bow_metrics = train_predict_evaluate_model(classifier=rf,\n",
    "                                           train_features=bow_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=bow_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_rf_bow = (rf_bow_predictions, rf_bow_metrics )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test on TFIDF features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1160    2]\n",
      " [ 217  360]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      1.00      0.91      1162\n",
      "           1       0.99      0.62      0.77       577\n",
      "\n",
      "    accuracy                           0.87      1739\n",
      "   macro avg       0.92      0.81      0.84      1739\n",
      "weighted avg       0.89      0.87      0.86      1739\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict and evaluate naive bayes\n",
    "mnb_tfidf_predictions, mnb_tfidf_metrics = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_nb_tfidf = (mnb_tfidf_predictions, mnb_tfidf_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1120   42]\n",
      " [  36  541]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.96      0.97      1162\n",
      "           1       0.93      0.94      0.93       577\n",
      "\n",
      "    accuracy                           0.96      1739\n",
      "   macro avg       0.95      0.95      0.95      1739\n",
      "weighted avg       0.96      0.96      0.96      1739\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_tfidf_predictions, dt_tfidf_metrics = train_predict_evaluate_model(classifier=dt,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_dt_tfidf = ( dt_tfidf_predictions, dt_tfidf_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1158    4]\n",
      " [  41  536]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98      1162\n",
      "           1       0.99      0.93      0.96       577\n",
      "\n",
      "    accuracy                           0.97      1739\n",
      "   macro avg       0.98      0.96      0.97      1739\n",
      "weighted avg       0.97      0.97      0.97      1739\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# predict and evaluate random forest\n",
    "rf_tfidf_predictions, rf_tfidf_metrics = train_predict_evaluate_model(classifier=rf,\n",
    "                                           train_features=tfidf_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=tfidf_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_rf_tfidf = ( rf_tfidf_predictions, rf_tfidf_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test on Binary features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1160    2]\n",
      " [ 157  420]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.88      1.00      0.94      1162\n",
      "           1       1.00      0.73      0.84       577\n",
      "\n",
      "    accuracy                           0.91      1739\n",
      "   macro avg       0.94      0.86      0.89      1739\n",
      "weighted avg       0.92      0.91      0.90      1739\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnb_binary_predictions, mnb_binary_metrics = train_predict_evaluate_model(classifier=mnb,\n",
    "                                           train_features=binary_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=binary_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_nb_binary = (mnb_binary_predictions, mnb_binary_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1123   39]\n",
      " [  42  535]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.97      0.97      1162\n",
      "           1       0.93      0.93      0.93       577\n",
      "\n",
      "    accuracy                           0.95      1739\n",
      "   macro avg       0.95      0.95      0.95      1739\n",
      "weighted avg       0.95      0.95      0.95      1739\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt_binary_predictions, dt_binary_metrics = train_predict_evaluate_model(classifier=dt,\n",
    "                                           train_features=binary_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=binary_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_dt_binary = (dt_binary_predictions, dt_binary_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1156    6]\n",
      " [  39  538]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98      1162\n",
      "           1       0.99      0.93      0.96       577\n",
      "\n",
      "    accuracy                           0.97      1739\n",
      "   macro avg       0.98      0.96      0.97      1739\n",
      "weighted avg       0.97      0.97      0.97      1739\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_binary_predictions, rf_binary_metrics = train_predict_evaluate_model(classifier=rf,\n",
    "                                           train_features=binary_train_features,\n",
    "                                           train_labels=train_labels,\n",
    "                                           test_features=binary_test_features,\n",
    "                                           test_labels=test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_rf_binary = (rf_binary_predictions, rf_binary_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Perfomance Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-aab5a6619488>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mperformance_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mme\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"bow\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"tfidf\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'performance_dict[\"{}\"][\"{}\"][\"{}\"] = {}_{}_metrics[\"{}\"]'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mme\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#Accuracy Matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
     ]
    }
   ],
   "source": [
    "# create a dictionary that stores all the accuracy information\n",
    "performance_dict = {}\n",
    "\n",
    "for me in [\"accuracy\", \"precision\", \"recall\", \"f1\"]:\n",
    "    performance_dict[me] = {}\n",
    "    for m in [\"mnb\",\"dt\",\"rf\"]:\n",
    "        performance_dict[me][m] = {}\n",
    "        for f in [\"bow\",\"tfidf\",\"binary\"]:\n",
    "            exec('performance_dict[\"{}\"][\"{}\"][\"{}\"] = {}_{}_metrics[\"{}\"]'.format(me, m, f, m, f, me))\n",
    "        \n",
    "#Accuracy Matrix\n",
    "print(\"\\n\\033[1;31mAccuracy Matrix\\n\\033[0m\")\n",
    "print(pd.DataFrame(performance_dict[\"accuracy\"]).rename(columns={\"mnb\":\"Naive Bayes\", \n",
    "                                            \"dt\":\"Decision Tree\", \n",
    "                                            \"rf\":\"Random Forest\"}, \n",
    "                                   index={\"bow\":\"Bag-of-words\", \n",
    "                                          \"tfidf\":\"TFIDF\", \n",
    "                                         \"binary\":\"Binary\" }))\n",
    "\n",
    "#Precision Matrix\n",
    "print(\"\\n\\033[1;31mPrecision Matrix\\n\\033[0m\")\n",
    "print(pd.DataFrame(performance_dict[\"precision\"]).rename(columns={\"mnb\":\"Naive Bayes\", \n",
    "                                            \"dt\":\"Decision Tree\", \n",
    "                                            \"rf\":\"Random Forest\"}, \n",
    "                                   index={\"bow\":\"Bag-of-words\", \n",
    "                                          \"tfidf\":\"TFIDF\", \n",
    "                                          \"binary\":\"Binary\"}))\n",
    "\n",
    "#Recall Matrix\n",
    "print(\"\\n\\033[1;31mRecall Matrix\\n\\033[0m\")\n",
    "print(pd.DataFrame(performance_dict[\"recall\"]).rename(columns={\"mnb\":\"Naive Bayes\", \n",
    "                                            \"dt\":\"Decision Tree\", \n",
    "                                            \"rf\":\"Random Forest\"}, \n",
    "                                   index={\"bow\":\"Bag-of-words\", \n",
    "                                          \"tfidf\":\"TFIDF\", \n",
    "                                         \"binary\":\"Binary\" }))\n",
    "\n",
    "#F1 Score Matrix\n",
    "print(\"\\n\\033[1;31mF1 Score Matrix\\n\\033[0m\")\n",
    "print(pd.DataFrame(performance_dict[\"f1\"]).rename(columns={\"mnb\":\"Naive Bayes\", \n",
    "                                            \"dt\":\"Decision Tree\", \n",
    "                                            \"rf\":\"Random Forest\"}, \n",
    "                                   index={\"bow\":\"Bag-of-words\", \n",
    "                                          \"tfidf\":\"TFIDF\", \n",
    "                                         \"binary\":\"Binary\"  }))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost for Bag of Words (Frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_nb_cost = matrix_nb_bow [1][0] * 5 + matrix_nb_bow[0][1] * 100\n",
    "bow_dt_cost = matrix_dt_bow [1][0] * 5 + matrix_dt_bow[0][1] * 100\n",
    "bow_rf_cost = matrix_rf_bow [1][0] * 5 + matrix_rf_bow[0][1] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost for TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_nb_cost = matrix_nb_tfidf[1][0] * 5 + matrix_nb_tfidf [0][1] * 100\n",
    "tfidf_dt_cost = matrix_dt_tfidf[1][0] * 5 + matrix_dt_tfidf [0][1] * 100\n",
    "tfidf_rf_cost = matrix_rf_tfidf[1][0] * 5 + matrix_rf_tfidf[0][1] * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost for Binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_nb_cost = matrix_nb_binary[1][0] * 5 + matrix_nb_binary [0][1] * 100\n",
    "binary_dt_cost = matrix_dt_binary[1][0] * 5 + matrix_dt_binary [0][1] * 100\n",
    "binary_rf_cost = matrix_rf_binary[1][0] * 5 + matrix_rf_binary [0][1] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bow_nb_cost =  1220\n",
      "bow_dt_cost =  4805\n",
      "bow_rf_cost =  590\n",
      "tfidf_nb_cost =  1285\n",
      "tfidf_dt_cost =  4380\n",
      "tfidf_rf_cost =  605\n",
      "binary_nb_cost =  985\n",
      "binary_dt_cost =  4110\n",
      "binary_rf_cost =  795\n"
     ]
    }
   ],
   "source": [
    "print('bow_nb_cost = ',bow_nb_cost)\n",
    "print('bow_dt_cost = ',bow_dt_cost)\n",
    "print('bow_rf_cost = ',bow_rf_cost)\n",
    "print('tfidf_nb_cost = ',tfidf_nb_cost)\n",
    "print('tfidf_dt_cost = ', tfidf_dt_cost)\n",
    "print('tfidf_rf_cost = ',tfidf_rf_cost)\n",
    "print('binary_nb_cost = ', binary_nb_cost)\n",
    "print('binary_dt_cost = ', binary_dt_cost)\n",
    "print('binary_rf_cost = ', binary_rf_cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
